{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1: Using DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Read in the csv datasets\n",
    "iris_path = 'D:\\lovestudy\\studybetter\\coding\\python\\Machine_Learning\\iris\\iris.csv'\n",
    "iris = pd.read_csv(iris_path)\n",
    "target_var = 'class'\n",
    "features = list(iris.columns)\n",
    "features.remove(target_var)\n",
    "# print(features)\n",
    "\n",
    "# Extract all features\n",
    "iris_class = iris[target_var].unique()\n",
    "# print(iris_class)\n",
    "\n",
    "# Encode the classes\n",
    "class_dict = dict(zip(iris_class, range(len(iris_class))))\n",
    "# print(class_dict)\n",
    "\n",
    "iris['target'] = iris[target_var].apply(lambda x: class_dict[x])\n",
    "# print(iris, type(iris))\n",
    "\n",
    "# one-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(list(class_dict.values()))\n",
    "transformed_labels = lb.transform(iris['target'])\n",
    "# print(transformed_labels)\n",
    "\n",
    "y_bin_labels = []\n",
    "for j in range(transformed_labels.shape[1]):\n",
    "    # print(i)\n",
    "    label = 'y' + str(j)\n",
    "    y_bin_labels.append(label)\n",
    "    iris[label] = transformed_labels[:, j]\n",
    "# print(iris)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    iris[features], iris[y_bin_labels], train_size=0.7, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a DNN model\n",
    "init = keras.initializers.glorot_uniform(seed=1)\n",
    "adam = keras.optimizers.Adam()\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(units=5, input_dim=4, kernel_initializer=init, activation='relu'))\n",
    "model.add(Dense(units=6, kernel_initializer=init, activation='relu'))\n",
    "model.add(Dense(units=3, kernel_initializer=init, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training..\n",
      "Epoch 1/100\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 1.0595 - acc: 0.3048\n",
      "Epoch 2/100\n",
      "105/105 [==============================] - 0s 945us/step - loss: 0.9624 - acc: 0.4857\n",
      "Epoch 3/100\n",
      "105/105 [==============================] - 0s 912us/step - loss: 0.8808 - acc: 0.6190\n",
      "Epoch 4/100\n",
      "105/105 [==============================] - 0s 907us/step - loss: 0.8282 - acc: 0.6667\n",
      "Epoch 5/100\n",
      "105/105 [==============================] - 0s 916us/step - loss: 0.7818 - acc: 0.7333\n",
      "Epoch 6/100\n",
      "105/105 [==============================] - 0s 869us/step - loss: 0.7434 - acc: 0.7238\n",
      "Epoch 7/100\n",
      "105/105 [==============================] - 0s 879us/step - loss: 0.7054 - acc: 0.7619\n",
      "Epoch 8/100\n",
      "105/105 [==============================] - 0s 860us/step - loss: 0.6564 - acc: 0.8476\n",
      "Epoch 9/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.6279 - acc: 0.8286\n",
      "Epoch 10/100\n",
      "105/105 [==============================] - 0s 817us/step - loss: 0.5988 - acc: 0.9048\n",
      "Epoch 11/100\n",
      "105/105 [==============================] - 0s 869us/step - loss: 0.5743 - acc: 0.9429\n",
      "Epoch 12/100\n",
      "105/105 [==============================] - 0s 888us/step - loss: 0.5546 - acc: 0.9238\n",
      "Epoch 13/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.5267 - acc: 0.9524\n",
      "Epoch 14/100\n",
      "105/105 [==============================] - 0s 902us/step - loss: 0.5048 - acc: 0.9619\n",
      "Epoch 15/100\n",
      "105/105 [==============================] - 0s 841us/step - loss: 0.4844 - acc: 0.9524\n",
      "Epoch 16/100\n",
      "105/105 [==============================] - 0s 893us/step - loss: 0.4621 - acc: 0.9619\n",
      "Epoch 17/100\n",
      "105/105 [==============================] - 0s 916us/step - loss: 0.4419 - acc: 0.9714\n",
      "Epoch 18/100\n",
      "105/105 [==============================] - 0s 902us/step - loss: 0.4226 - acc: 0.9714\n",
      "Epoch 19/100\n",
      "105/105 [==============================] - 0s 898us/step - loss: 0.4049 - acc: 0.9714\n",
      "Epoch 20/100\n",
      "105/105 [==============================] - 0s 926us/step - loss: 0.3934 - acc: 0.9619\n",
      "Epoch 21/100\n",
      "105/105 [==============================] - 0s 912us/step - loss: 0.3677 - acc: 0.9810\n",
      "Epoch 22/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3599 - acc: 0.9714\n",
      "Epoch 23/100\n",
      "105/105 [==============================] - 0s 926us/step - loss: 0.3572 - acc: 0.9524\n",
      "Epoch 24/100\n",
      "105/105 [==============================] - 0s 940us/step - loss: 0.3341 - acc: 0.9619\n",
      "Epoch 25/100\n",
      "105/105 [==============================] - 0s 949us/step - loss: 0.3158 - acc: 0.9619\n",
      "Epoch 26/100\n",
      "105/105 [==============================] - 0s 926us/step - loss: 0.3050 - acc: 0.9619\n",
      "Epoch 27/100\n",
      "105/105 [==============================] - 0s 959us/step - loss: 0.2909 - acc: 0.9714\n",
      "Epoch 28/100\n",
      "105/105 [==============================] - 0s 949us/step - loss: 0.2938 - acc: 0.9524\n",
      "Epoch 29/100\n",
      "105/105 [==============================] - 0s 935us/step - loss: 0.2722 - acc: 0.9429\n",
      "Epoch 30/100\n",
      "105/105 [==============================] - 0s 822us/step - loss: 0.2657 - acc: 0.9619\n",
      "Epoch 31/100\n",
      "105/105 [==============================] - 0s 850us/step - loss: 0.2560 - acc: 0.9714\n",
      "Epoch 32/100\n",
      "105/105 [==============================] - 0s 836us/step - loss: 0.2360 - acc: 0.9619\n",
      "Epoch 33/100\n",
      "105/105 [==============================] - 0s 883us/step - loss: 0.2314 - acc: 0.9714\n",
      "Epoch 34/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2202 - acc: 0.9524\n",
      "Epoch 35/100\n",
      "105/105 [==============================] - 0s 973us/step - loss: 0.2266 - acc: 0.9429\n",
      "Epoch 36/100\n",
      "105/105 [==============================] - 0s 983us/step - loss: 0.2095 - acc: 0.9619\n",
      "Epoch 37/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2001 - acc: 0.9810\n",
      "Epoch 38/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1927 - acc: 0.9810\n",
      "Epoch 39/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1884 - acc: 0.9810\n",
      "Epoch 40/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1843 - acc: 0.9714\n",
      "Epoch 41/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1783 - acc: 0.9619\n",
      "Epoch 42/100\n",
      "105/105 [==============================] - 0s 945us/step - loss: 0.1699 - acc: 0.9714\n",
      "Epoch 43/100\n",
      "105/105 [==============================] - 0s 973us/step - loss: 0.1683 - acc: 0.9619\n",
      "Epoch 44/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1577 - acc: 0.9619\n",
      "Epoch 45/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1573 - acc: 0.9619\n",
      "Epoch 46/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1473 - acc: 0.9714\n",
      "Epoch 47/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1506 - acc: 0.9714\n",
      "Epoch 48/100\n",
      "105/105 [==============================] - 0s 931us/step - loss: 0.1446 - acc: 0.9810\n",
      "Epoch 49/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1532 - acc: 0.9619\n",
      "Epoch 50/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1289 - acc: 0.9714\n",
      "Epoch 51/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1373 - acc: 0.9619\n",
      "Epoch 52/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1264 - acc: 0.9714\n",
      "Epoch 53/100\n",
      "105/105 [==============================] - 0s 954us/step - loss: 0.1263 - acc: 0.9810\n",
      "Epoch 54/100\n",
      "105/105 [==============================] - 0s 921us/step - loss: 0.1252 - acc: 0.9714\n",
      "Epoch 55/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1209 - acc: 0.9714\n",
      "Epoch 56/100\n",
      "105/105 [==============================] - 0s 997us/step - loss: 0.1205 - acc: 0.9619\n",
      "Epoch 57/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1101 - acc: 0.9905\n",
      "Epoch 58/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1150 - acc: 0.9714\n",
      "Epoch 59/100\n",
      "105/105 [==============================] - 0s 973us/step - loss: 0.1210 - acc: 0.9333\n",
      "Epoch 60/100\n",
      "105/105 [==============================] - 0s 945us/step - loss: 0.1016 - acc: 0.9810\n",
      "Epoch 61/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1205 - acc: 0.9524\n",
      "Epoch 62/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1121 - acc: 0.9619\n",
      "Epoch 63/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1032 - acc: 0.9619\n",
      "Epoch 64/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1106 - acc: 0.9524\n",
      "Epoch 65/100\n",
      "105/105 [==============================] - 0s 959us/step - loss: 0.0849 - acc: 0.9810\n",
      "Epoch 66/100\n",
      "105/105 [==============================] - 0s 869us/step - loss: 0.0951 - acc: 0.9619\n",
      "Epoch 67/100\n",
      "105/105 [==============================] - 0s 822us/step - loss: 0.0984 - acc: 0.9810\n",
      "Epoch 68/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0922 - acc: 0.9810\n",
      "Epoch 69/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0808 - acc: 0.9810\n",
      "Epoch 70/100\n",
      "105/105 [==============================] - 0s 940us/step - loss: 0.0869 - acc: 0.9810\n",
      "Epoch 71/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0999 - acc: 0.9905\n",
      "Epoch 72/100\n",
      "105/105 [==============================] - 0s 935us/step - loss: 0.0840 - acc: 0.9714\n",
      "Epoch 73/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0747 - acc: 0.9810\n",
      "Epoch 74/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0802 - acc: 0.9810\n",
      "Epoch 75/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0774 - acc: 0.9810\n",
      "Epoch 76/100\n",
      "105/105 [==============================] - 0s 992us/step - loss: 0.0799 - acc: 0.9905\n",
      "Epoch 77/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0765 - acc: 0.9810\n",
      "Epoch 78/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0759 - acc: 0.9810\n",
      "Epoch 79/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0783 - acc: 0.9714\n",
      "Epoch 80/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0723 - acc: 0.9714\n",
      "Epoch 81/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0684 - acc: 0.9810\n",
      "Epoch 82/100\n",
      "105/105 [==============================] - 0s 931us/step - loss: 0.0647 - acc: 0.9810\n",
      "Epoch 83/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0862 - acc: 0.9619\n",
      "Epoch 84/100\n",
      "105/105 [==============================] - 0s 874us/step - loss: 0.0723 - acc: 0.9810\n",
      "Epoch 85/100\n",
      "105/105 [==============================] - 0s 751us/step - loss: 0.0731 - acc: 0.9714\n",
      "Epoch 86/100\n",
      "105/105 [==============================] - 0s 898us/step - loss: 0.0664 - acc: 0.9810\n",
      "Epoch 87/100\n",
      "105/105 [==============================] - 0s 940us/step - loss: 0.0668 - acc: 0.9905\n",
      "Epoch 88/100\n",
      "105/105 [==============================] - 0s 902us/step - loss: 0.0660 - acc: 0.9810\n",
      "Epoch 89/100\n",
      "105/105 [==============================] - 0s 841us/step - loss: 0.0628 - acc: 0.9810\n",
      "Epoch 90/100\n",
      "105/105 [==============================] - 0s 874us/step - loss: 0.0627 - acc: 0.9810\n",
      "Epoch 91/100\n",
      "105/105 [==============================] - 0s 888us/step - loss: 0.0672 - acc: 0.9619\n",
      "Epoch 92/100\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0611 - acc: 0.9905\n",
      "Epoch 93/100\n",
      "105/105 [==============================] - 0s 907us/step - loss: 0.0657 - acc: 0.9810\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - 0s 808us/step - loss: 0.0569 - acc: 0.9905\n",
      "Epoch 95/100\n",
      "105/105 [==============================] - 0s 879us/step - loss: 0.0614 - acc: 0.9810\n",
      "Epoch 96/100\n",
      "105/105 [==============================] - 0s 864us/step - loss: 0.0603 - acc: 0.9810\n",
      "Epoch 97/100\n",
      "105/105 [==============================] - 0s 926us/step - loss: 0.0550 - acc: 0.9810\n",
      "Epoch 98/100\n",
      "105/105 [==============================] - 0s 836us/step - loss: 0.0903 - acc: 0.9524\n",
      "Epoch 99/100\n",
      "105/105 [==============================] - 0s 836us/step - loss: 0.0570 - acc: 0.9905\n",
      "Epoch 100/100\n",
      "105/105 [==============================] - 0s 846us/step - loss: 0.0615 - acc: 0.9714\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "print('Start training..')\n",
    "h = model.fit(train_x, train_y, batch_size=1,\n",
    "              epochs=100, shuffle=True, verbose=1)\n",
    "print('Training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test data: loss = 0.108226 accuracy = 95.56% \n"
     ]
    }
   ],
   "source": [
    "eval_m = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Evaluation on test data: loss = %0.6f accuracy = %0.2f%% \" %\n",
    "      (eval_m[0], eval_m[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2: Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.25)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='multinomial').fit(X, y)\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 防止过拟合的方法：\n",
    "## 1.避免使用高阶特征\n",
    "## 2.引入正则项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
